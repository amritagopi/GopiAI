#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–æ–≤ –≤ RAG —Å–∏—Å—Ç–µ–º—É GopiAI
–†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–æ–π —Ñ–∞–π–ª –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –∫—É—Å–æ—á–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∏—Ä—É–µ—Ç –∏—Ö
"""

import json
import re
from pathlib import Path
from datetime import datetime


def split_chat_into_topics(chat_content: str) -> list:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —á–∞—Ç –Ω–∞ —Ç–µ–º—ã –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
    
    # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ —Ç–µ–º (—Å—Ç—Ä–æ–∫–∏ –±–µ–∑ "user" –∏–ª–∏ "ChatGPT" –≤ –Ω–∞—á–∞–ª–µ)
    lines = chat_content.split('\n')
    topics = []
    current_topic = {'title': '', 'content': ''}
    current_content = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–µ–º—ã (–æ–±—ã—á–Ω–æ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ user/ChatGPT)
        if (not line.startswith('user') and 
            not line.startswith('ChatGPT') and 
            not line.startswith('tool') and
            not line.startswith('```') and
            not line.startswith('#') and
            len(current_content) == 0):
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é —Ç–µ–º—É –µ—Å–ª–∏ –±—ã–ª–∞
            if current_topic['content']:
                topics.append({
                    'title': current_topic['title'],
                    'content': current_topic['content'],
                    'chunk_size': len(current_topic['content'])
                })
            
            # –ù–∞—á–∏–Ω–∞–µ–º –Ω–æ–≤—É—é —Ç–µ–º—É
            current_topic = {
                'title': line,
                'content': ''
            }
            current_content = []
            
        else:
            current_content.append(line)
            current_topic['content'] = '\n'.join(current_content)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Ç–µ–º—É
    if current_topic['content']:
        topics.append({
            'title': current_topic['title'],
            'content': current_topic['content'],
            'chunk_size': len(current_topic['content'])
        })
    
    return topics


def chunk_large_topics(topics: list, max_chunk_size: int = 3000) -> list:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç –±–æ–ª—å—à–∏–µ —Ç–µ–º—ã –Ω–∞ –±–æ–ª–µ–µ –º–µ–ª–∫–∏–µ –∫—É—Å–æ—á–∫–∏"""
    
    chunked_topics = []
    
    for topic in topics:
        if topic['chunk_size'] <= max_chunk_size:
            chunked_topics.append(topic)
        else:
            # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à—É—é —Ç–µ–º—É –Ω–∞ —á–∞—Å—Ç–∏
            content = topic['content']
            lines = content.split('\n')
            
            chunk_lines = []
            current_chunk_size = 0
            chunk_num = 1
            
            for line in lines:
                if current_chunk_size + len(line) > max_chunk_size and chunk_lines:
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—É—â–∏–π –∫—É—Å–æ—á–µ–∫
                    chunked_topics.append({
                        'title': f"{topic['title']} (—á–∞—Å—Ç—å {chunk_num})",
                        'content': '\n'.join(chunk_lines),
                        'chunk_size': current_chunk_size
                    })
                    
                    chunk_lines = [line]
                    current_chunk_size = len(line)
                    chunk_num += 1
                else:
                    chunk_lines.append(line)
                    current_chunk_size += len(line) + 1
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–π –∫—É—Å–æ—á–µ–∫
            if chunk_lines:
                chunked_topics.append({
                    'title': f"{topic['title']} (—á–∞—Å—Ç—å {chunk_num})" if chunk_num > 1 else topic['title'],
                    'content': '\n'.join(chunk_lines),
                    'chunk_size': current_chunk_size
                })
    
    return chunked_topics


def add_chunks_to_rag(chunks: list) -> bool:
    """–î–æ–±–∞–≤–ª—è–µ—Ç –∫—É—Å–æ—á–∫–∏ –≤ RAG —Å–∏—Å—Ç–µ–º—É —á–µ—Ä–µ–∑ —Ñ–∞–π–ª–æ–≤—É—é —Å–∏—Å—Ç–µ–º—É"""
    
    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —á–∞—Ç–æ–≤ RAG —Å–∏—Å—Ç–µ–º—ã
    rag_chats_file = Path("/home/amritagopi/GopiAI/GopiAI-CrewAI/memory/chats.json")
    
    try:
        # –ß–∏—Ç–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —á–∞—Ç—ã
        if rag_chats_file.exists():
            with open(rag_chats_file, 'r', encoding='utf-8') as f:
                existing_chats = json.load(f)
        else:
            existing_chats = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤—ã–µ –∫—É—Å–æ—á–∫–∏
        for i, chunk in enumerate(chunks):
            chat_entry = {
                "id": f"imported_chat_{datetime.now().strftime('%Y%m%d')}_{i+1}",
                "timestamp": datetime.now().isoformat(),
                "role": "user",
                "content": f"–¢–µ–º–∞: {chunk['title']}\n\n{chunk['content']}",
                "metadata": {
                    "source": "imported_chat_history", 
                    "topic": chunk['title'],
                    "chunk_size": chunk['chunk_size'],
                    "imported": True
                }
            }
            existing_chats.append(chat_entry)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        rag_chats_file.parent.mkdir(parents=True, exist_ok=True)
        with open(rag_chats_file, 'w', encoding='utf-8') as f:
            json.dump(existing_chats, f, ensure_ascii=False, indent=2)
        
        print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(chunks)} –∫—É—Å–æ—á–∫–æ–≤ –≤ {rag_chats_file}")
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤ RAG: {e}")
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    chat_file = Path("/home/amritagopi/GopiAI/Chat_for_editing_chunks.txt")
    
    if not chat_file.exists():
        print(f"‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {chat_file}")
        return
    
    print(f"üìñ –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª: {chat_file}")
    with open(chat_file, 'r', encoding='utf-8') as f:
        chat_content = f.read()
    
    print(f"üìù –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(chat_content)} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–µ–º—ã
    print("üîç –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Ç–µ–º—ã...")
    topics = split_chat_into_topics(chat_content)
    print(f"üìã –ù–∞–π–¥–µ–Ω–æ —Ç–µ–º: {len(topics)}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —Ç–µ–º–∞–º
    for i, topic in enumerate(topics[:10]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10
        print(f"  {i+1}. {topic['title'][:50]}... ({topic['chunk_size']} —Å–∏–º–≤–æ–ª–æ–≤)")
    
    if len(topics) > 10:
        print(f"  ... –∏ –µ—â–µ {len(topics) - 10} —Ç–µ–º")
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Ç–µ–º—ã –Ω–∞ –∫—É—Å–æ—á–∫–∏
    print("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∞–µ–º –±–æ–ª—å—à–∏–µ —Ç–µ–º—ã...")
    chunks = chunk_large_topics(topics, max_chunk_size=3000)
    print(f"üì¶ –ò—Ç–æ–≥–æ–≤—ã—Ö –∫—É—Å–æ—á–∫–æ–≤: {len(chunks)}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–∑–º–µ—Ä–æ–≤
    total_chars = sum(chunk['chunk_size'] for chunk in chunks)
    avg_size = total_chars // len(chunks)
    max_size = max(chunk['chunk_size'] for chunk in chunks)
    print(f"üìä –°—Ä–µ–¥–Ω–∏–π —Ä–∞–∑–º–µ—Ä –∫—É—Å–æ—á–∫–∞: {avg_size} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä: {max_size} —Å–∏–º–≤–æ–ª–æ–≤")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ RAG
    print("üíæ –î–æ–±–∞–≤–ª—è–µ–º –≤ RAG —Å–∏—Å—Ç–µ–º—É...")
    success = add_chunks_to_rag(chunks)
    
    if success:
        print("üéâ –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–æ–≤ —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ RAG!")
        print("üîÑ –¢–µ–ø–µ—Ä—å –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–µ—Ä –¥–ª—è –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏")
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–±–∞–≤–∏—Ç—å –≤ RAG")


if __name__ == "__main__":
    main()