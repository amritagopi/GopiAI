"""
CrewAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π LLM –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π code_execution
"""
import logging
from typing import Any, List, Optional, Iterator
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.outputs import ChatResult, ChatGeneration
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from pydantic import PrivateAttr

import google.generativeai as genai

logger = logging.getLogger(__name__)

class CrewAIGeminiLLM(BaseChatModel):
    """
    CrewAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π Gemini LLM —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π code_execution
    """
    
    model: str = "gemini-2.5-flash"
    temperature: float = 0.7
    enable_code_execution: bool = True
    _gemini_provider: Any = PrivateAttr()
    
    def __init__(
        self, 
        model: str = "gemini-2.5-flash",
        temperature: float = 0.7,
        enable_code_execution: bool = True,
        **kwargs
    ):
        # –í–ê–ñ–ù–û: –£–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –∏–º—è –º–æ–¥–µ–ª–∏ –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –ø—É—Ç–∞–Ω–∏—Ü—ã —Å Vertex AI
        corrected_model = f"google/{model}" if not model.startswith("google/") else model
        
        super().__init__(model=corrected_model, temperature=temperature, **kwargs)
        
        self.enable_code_execution = enable_code_execution
        self._original_model = model  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è Gemini SDK
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Gemini –ø—Ä–æ–≤–∞–π–¥–µ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π code execution
        import os
        genai.configure(api_key=os.getenv('GEMINI_API_KEY'))
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π code execution
        generation_config = genai.types.GenerationConfig(
            temperature=self.temperature
        )
        
        # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π code execution
        tools = [genai.protos.Tool(code_execution={})] if self.enable_code_execution else None
        
        self._gemini_model = genai.GenerativeModel(
            model_name=self._original_model,
            generation_config=generation_config,
            tools=tools
        )
        
        logger.info(f"üöÄ CrewAI Gemini LLM –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (model: {corrected_model}, code_execution: {self.enable_code_execution})")
    
    @property
    def _llm_type(self) -> str:
        return "crewai_gemini"
    
    def _generate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è CrewAI"""
        try:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –µ–¥–∏–Ω—ã–π —Ç–µ–∫—Å—Ç
            prompt_parts = []
            for message in messages:
                if isinstance(message, HumanMessage):
                    prompt_parts.append(f"Human: {message.content}")
                elif isinstance(message, AIMessage):
                    prompt_parts.append(f"Assistant: {message.content}")
                else:
                    prompt_parts.append(f"{message.content}")
            
            prompt = "\n".join(prompt_parts)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Gemini –º–æ–¥–µ–ª—å
            response = self._gemini_model.generate_content(prompt)
            response_text = response.text
            
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            message = AIMessage(content=response_text)
            generation = ChatGeneration(message=message)
            
            return ChatResult(generations=[generation])
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ CrewAI Gemini LLM: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—à–∏–±–∫—É –∫–∞–∫ —Å–æ–æ–±—â–µ–Ω–∏–µ
            error_message = AIMessage(
                content=f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞: {e}"
            )
            generation = ChatGeneration(message=error_message)
            return ChatResult(generations=[generation])
    
    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> ChatResult:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è (–ø–æ–∫–∞ –¥–µ–ª–µ–≥–∏—Ä—É–µ–º –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é)"""
        return self._generate(messages, stop, run_manager, **kwargs)
    
    def _stream(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> Iterator[ChatGeneration]:
        """Streaming –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –ø–æ–∫–∞"""
        result = self._generate(messages, stop, run_manager, **kwargs)
        yield result.generations[0]

def create_crewai_gemini_llm(
    model: str = "gemini-2.5-flash", 
    enable_code_execution: bool = True,
    temperature: float = 0.7
) -> CrewAIGeminiLLM:
    """–§–∞–±—Ä–∏—á–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è CrewAI Gemini LLM"""
    return CrewAIGeminiLLM(
        model=model,
        temperature=temperature,
        enable_code_execution=enable_code_execution
    )